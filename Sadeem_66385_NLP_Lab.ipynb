{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. Tokenization"
      ],
      "metadata": {
        "id": "iWNRjR5ZQeXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.chunk import RegexpParser\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G5yjikfRFWO",
        "outputId": "7c20fa42-26ad-4761-ad41-0cf7b48e0064"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to perform word tokenization using NLTK"
      ],
      "metadata": {
        "id": "36kxC2zXQjWC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyQWjoSKQT_B",
        "outputId": "86bf84e2-663c-47c9-b610-847f42aad7bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Riphah', 'University', 'is', 'located', 'in', 'Islamabad', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Riphah University is located in Islamabad.\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to perform sentence tokenization using NLTK"
      ],
      "metadata": {
        "id": "h1fqXJ5DRUGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "sentences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_ULEJMORXTe",
        "outputId": "496f3b11-10b2-49cd-f770-dc3f827abbfb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Riphah University is located in Islamabad.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Stopword Removal\n"
      ],
      "metadata": {
        "id": "oh2bpZHzRakV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to eliminate stopwords using NLTK."
      ],
      "metadata": {
        "id": "Xvy3vF5rRxtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_words = [w for w in words if w.lower() not in stop_words]\n",
        "filtered_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbRE9APkRopo",
        "outputId": "5b0c146e-8615-4058-a253-4538a372974b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Riphah', 'University', 'located', 'Islamabad', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Stemming"
      ],
      "metadata": {
        "id": "SsS5gxAMR6as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to perform stemming (Porter or Snowball) using NLTK."
      ],
      "metadata": {
        "id": "gR-wE5d6R77S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "snow = SnowballStemmer(\"english\")\n",
        "\n",
        "stemmed_porter = [ps.stem(w) for w in words]\n",
        "stemmed_snowball = [snow.stem(w) for w in words]\n",
        "\n",
        "stemmed_porter, stemmed_snowball\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyFTHqC8R3aw",
        "outputId": "fcd1732d-e810-4786-9366-476995a56563"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['riphah', 'univers', 'is', 'locat', 'in', 'islamabad', '.'],\n",
              " ['riphah', 'univers', 'is', 'locat', 'in', 'islamabad', '.'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Parts of Speech Tagging"
      ],
      "metadata": {
        "id": "OwsLW4-9SKFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to perform POS tagging using NLTK"
      ],
      "metadata": {
        "id": "c7CNtwL3SSf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "pos_result = pos_tag(words)\n",
        "pos_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHB7b8wXSOvn",
        "outputId": "cd76c2d8-38d3-40d9-ffe9-35bc4710af9d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Riphah', 'NNP'),\n",
              " ('University', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('located', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('Islamabad', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Lemmatization\n"
      ],
      "metadata": {
        "id": "1KUA5Hs2Seme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to perform lemmatization using WordNetLemmatizer."
      ],
      "metadata": {
        "id": "WAvyDaB-SjZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(w) for w in words]\n",
        "lemmatized_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZJNZqypSmiY",
        "outputId": "71dea368-668e-4e6c-96b6-8493b79084e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Riphah', 'University', 'is', 'located', 'in', 'Islamabad', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Chunking\n"
      ],
      "metadata": {
        "id": "X5ikekjKSrlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program for chunking using NLTK‚Äôs RegEx chunk grammar"
      ],
      "metadata": {
        "id": "EOy_OBuDSu-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "parser = RegexpParser(grammar)\n",
        "\n",
        "chunked_output = parser.parse(tagged)\n",
        "chunked_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "t_BXAddrSy6O",
        "outputId": "17eefd8b-487d-4b56-a712-0ef6e61f8828"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/67.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN')]), Tree('NP', [('fox', 'NN')]), ('jumps', 'VBZ'), ('over', 'IN'), Tree('NP', [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,424.0,168.0\" width=\"424px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"35.8491%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"26.3158%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.1579%\" y1=\"20px\" y2=\"48px\" /><svg width=\"36.8421%\" x=\"26.3158%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">quick</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"44.7368%\" y1=\"20px\" y2=\"48px\" /><svg width=\"36.8421%\" x=\"63.1579%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">brown</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.5789%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.9245%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.43396%\" x=\"35.8491%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">fox</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.566%\" y1=\"20px\" y2=\"48px\" /><svg width=\"13.2075%\" x=\"45.283%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">jumps</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.8868%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.3208%\" x=\"58.4906%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">over</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.1509%\" y1=\"20px\" y2=\"48px\" /><svg width=\"30.1887%\" x=\"69.8113%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"31.25%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"37.5%\" x=\"31.25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">lazy</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /><svg width=\"31.25%\" x=\"68.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dog</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.375%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.9057%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Named Entity Recognition\n"
      ],
      "metadata": {
        "id": "iv-ErYSqS_BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program for NER using NLTK‚Äôs ne_chunk()."
      ],
      "metadata": {
        "id": "S1M-6joQTBu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"Albert Einstein was born in Germany.\"\n",
        "\n",
        "tokens = word_tokenize(text2)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "ner_output = ne_chunk(tagged)\n",
        "ner_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "doWm6-sDTFFh",
        "outputId": "6b837098-0340-4183-911c-6f0348657463"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('PERSON', [('Albert', 'NNP')]), Tree('PERSON', [('Einstein', 'NNP')]), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), Tree('GPE', [('Germany', 'NNP')]), ('.', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,360.0,168.0\" width=\"360px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"17.7778%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Albert</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.88889%\" y1=\"20px\" y2=\"48px\" /><svg width=\"22.2222%\" x=\"17.7778%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Einstein</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.8889%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.1111%\" x=\"40%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.5556%\" y1=\"20px\" y2=\"48px\" /><svg width=\"13.3333%\" x=\"51.1111%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">born</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.7778%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.88889%\" x=\"64.4444%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.8889%\" y1=\"20px\" y2=\"48px\" /><svg width=\"20%\" x=\"73.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Germany</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.66667%\" x=\"93.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.6667%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. TF‚ÄìIDF Calculation\n",
        "##‚Ä¢ Write a Python program to compute:\n",
        "##o Term Frequency (TF)\n",
        "##o Inverse Document Frequency (IDF)\n",
        "##o TF‚ÄìIDF values"
      ],
      "metadata": {
        "id": "oeJl5CfqTOzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"AI is the future of technology.\",\n",
        "    \"Machine learning is part of AI.\",\n",
        "    \"Deep learning advances AI research.\"\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "D8D9jCE_TiXf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üîπ Step 1 ‚Äî Compute Term Frequency (TF)"
      ],
      "metadata": {
        "id": "X3gfdEa4T511"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "tf_matrix = cv.fit_transform(docs)\n",
        "\n",
        "print(\"TF Matrix:\")\n",
        "print(tf_matrix.toarray())\n",
        "print(\"\\nVocabulary:\")\n",
        "print(cv.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNLmyacRT3ly",
        "outputId": "a8763234-5055-41b2-c16f-9a1f04ea2913"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF Matrix:\n",
            "[[0 1 0 1 1 0 0 1 0 0 1 1]\n",
            " [0 1 0 0 1 1 1 1 1 0 0 0]\n",
            " [1 1 1 0 0 1 0 0 0 1 0 0]]\n",
            "\n",
            "Vocabulary:\n",
            "['advances' 'ai' 'deep' 'future' 'is' 'learning' 'machine' 'of' 'part'\n",
            " 'research' 'technology' 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üîπ Step 2 ‚Äî Compute Inverse Document Frequency (IDF)"
      ],
      "metadata": {
        "id": "X0uFb6VFT9_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidf_transformer.fit(tf_matrix)\n",
        "\n",
        "print(\"\\nIDF Values:\")\n",
        "for word, idf_val in zip(cv.get_feature_names_out(), tfidf_transformer.idf_):\n",
        "    print(word, \":\", idf_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p914A9YYUCM6",
        "outputId": "69bd4d9c-3d8c-4233-ff8c-c1d5f6e7994b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IDF Values:\n",
            "advances : 1.6931471805599454\n",
            "ai : 1.0\n",
            "deep : 1.6931471805599454\n",
            "future : 1.6931471805599454\n",
            "is : 1.2876820724517808\n",
            "learning : 1.2876820724517808\n",
            "machine : 1.6931471805599454\n",
            "of : 1.2876820724517808\n",
            "part : 1.6931471805599454\n",
            "research : 1.6931471805599454\n",
            "technology : 1.6931471805599454\n",
            "the : 1.6931471805599454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üîπ Step 3 ‚Äî Compute TF-IDF Values"
      ],
      "metadata": {
        "id": "9-vWWAypUD2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix = tfidf_transformer.transform(tf_matrix)\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bccaRGl3UG0k",
        "outputId": "978d95f5-bc76-408d-efb5-f5f588e55a69"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.27824521 0.         0.4711101  0.35829137 0.\n",
            "  0.         0.35829137 0.         0.         0.4711101  0.4711101 ]\n",
            " [0.         0.2922544  0.         0.         0.37633075 0.37633075\n",
            "  0.49482971 0.37633075 0.49482971 0.         0.         0.        ]\n",
            " [0.50461134 0.29803159 0.50461134 0.         0.         0.38376993\n",
            "  0.         0.         0.         0.50461134 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus 1: Sentiment Analysis\n",
        "##‚Ä¢ Write Python code to perform sentiment analysis using NLP (VADER or TextBlob)"
      ],
      "metadata": {
        "id": "jp0t2QAtULTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "sentence = \"I absolutely love Artificial Intelligence!\"\n",
        "sia.polarity_scores(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzhRgoymUR7-",
        "outputId": "dae4524f-aa50-47b2-eec9-435e7ed46294"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.197, 'pos': 0.803, 'compound': 0.8461}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus 2: Spam Filter\n",
        "##‚Ä¢ Write Python code to develop a Spam Detection model using NLP."
      ],
      "metadata": {
        "id": "cQA_QPAqUVK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "data = [\n",
        "    (\"Win a free iPhone now!\", \"spam\"),\n",
        "    (\"Your appointment is confirmed.\", \"ham\"),\n",
        "    (\"Congratulations! You won a lottery!\", \"spam\"),\n",
        "    (\"Please submit the assignment today.\", \"ham\")\n",
        "]\n",
        "\n",
        "texts = [x[0] for x in data]\n",
        "labels = [x[1] for x in data]\n",
        "\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(texts)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "accuracy_score(y_test, pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RczveUlzUacf",
        "outputId": "478c015f-8525-49d2-9b3e-539cf777ebf3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus 3: Fake News Detection\n",
        "##‚Ä¢ Write Python code to build a Fake News Classification model using NLP and ML."
      ],
      "metadata": {
        "id": "-j5AnVvGUc8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "docs = [\n",
        "    \"Breaking: Aliens landed in New York!\",  # fake\n",
        "    \"PM announces new economic reforms.\",    # real\n",
        "    \"Scientists discover cure for aging!\",   # fake\n",
        "    \"Weather report predicts rain tomorrow.\" # real\n",
        "]\n",
        "\n",
        "labels = [\"fake\", \"real\", \"fake\", \"real\"]\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(docs)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25)\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "preds = classifier.predict(X_test)\n",
        "accuracy_score(y_test, preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-Ypyk2JUihb",
        "outputId": "f9d4f802-dd1f-4ecd-def2-1330963b0743"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}